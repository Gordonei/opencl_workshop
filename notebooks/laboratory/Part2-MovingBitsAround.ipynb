{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Part 2 - Moving Bits Around (with OpenCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Library Import\n",
    "Before doing anything else, we need to import [PyOpenCL](https://documen.tician.de/pyopencl/) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyopencl\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to use Matplotlib to visualise performance improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up platforms, devices and context\n",
    "We're going to setup the devices and context as explicit objects because we might want to interogate their runtime information.\n",
    "\n",
    "*NB* First, for the devices, we use ordinary index-based accessing of the devices. Then, I use Python's ability to unpack collections directly into variables. It takes a little getting used to, but this is the best example of dynamic programming languages ability to support succinct yet clear coding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "platforms = pyopencl.get_platforms()\n",
    "\n",
    "# Devices\n",
    "devices = [platform.get_devices()[0] for platform in platforms]\n",
    "nvidia_device = devices[0]\n",
    "intel_device = devices[1]\n",
    "\n",
    "# Contexts\n",
    "contexts = [pyopencl.Context(devices=[device]) for device in (nvidia_device,intel_device)]\n",
    "nvidia_context,intel_context = contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Communicating between Host and Device\n",
    "### Setting up the program\n",
    "1. Using the OpenCL source in `part_2.cl`, create a new program.\n",
    "2. Compile the programs and setup the command queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"part_2.cl\",\"r\") as program_source_file:\n",
    "    program_source = program_source_file.read()\n",
    "\n",
    "nvidia_program_source,intel_program_source = [pyopencl.Program(context,program_source) \n",
    "                                              for context \n",
    "                                              in (nvidia_context,intel_context)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_program,intel_program = [program.build()\n",
    "                                for program\n",
    "                                in (nvidia_program_source,intel_program_source)]\n",
    "\n",
    "nvidia_queue = pyopencl.CommandQueue(nvidia_context)\n",
    "intel_queue = pyopencl.CommandQueue(intel_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the global memory buffers\n",
    "1. Generating the source data\n",
    "2. Creating the memories of the right size within the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_arrays(M=2**10,N=2**8,datatype=numpy.int32,b_colwise = False):\n",
    "    #Setting up memory\n",
    "    a = (numpy.random.randint(0,10,size=(M,N))).astype(dt)\n",
    "    b = (numpy.random.randint(0,10,size=(N,M))).astype(dt)\n",
    "    if(b_colwise): b = numpy.asfortranarray(b)\n",
    "    \n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_buffers(context,datatype_size,M=2**10,N=2**8):\n",
    "    ro_mem_flags = pyopencl.mem_flags.READ_ONLY\n",
    "    \n",
    "    a_buffer = pyopencl.Buffer(context,\n",
    "                               flags=ro_mem_flags,\n",
    "                               size=M*N*datatype_size)              \n",
    "    b_buffer = pyopencl.Buffer(context,\n",
    "                               flags=ro_mem_flags,\n",
    "                               size=M*N*datatype_size)\n",
    "    \n",
    "    wo_mem_flags = pyopencl.mem_flags.WRITE_ONLY\n",
    "    c_buffer = pyopencl.Buffer(context,\n",
    "                               flags=wo_mem_flags,\n",
    "                               size=M*M*datatype_size)\n",
    "    \n",
    "    return a_buffer,b_buffer,c_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating buffers\n",
    "dt = numpy.int32 #The datatype we're using\n",
    "data_arrays = create_arrays(datatype=dt)\n",
    "\n",
    "datatype_size = numpy.dtype(dt).itemsize\n",
    "nvidia_buffers = create_buffers(nvidia_context,datatype_size)\n",
    "intel_buffers = create_buffers(intel_context,datatype_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the program\n",
    "### Defining the host program\n",
    "We want to perform a matrix multiplication:\n",
    "$$\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}$$\n",
    "\n",
    "We create a function for running the program that takes the source data, the memory buffers we've created, the command queue, and the program kernel we're going to use. \n",
    "\n",
    "Optional arguments let us change the size of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opencl_dot_product(data_arrays,buffers,queue,kernel,M=2**10,N=2**8):\n",
    "    a,b = data_arrays\n",
    "    a_buffer,b_buffer,c_buffer = buffers\n",
    "    M,N = numpy.int32(M),numpy.int32(N) #converting to the exepcted type for OpenCL\n",
    "    \n",
    "    #copying data onto device\n",
    "    copyon_events = []\n",
    "        \n",
    "    copyon_events += [pyopencl.enqueue_copy(queue,\n",
    "                                            src=a,\n",
    "                                            dest=a_buffer)]\n",
    "    copyon_events += [pyopencl.enqueue_copy(queue,\n",
    "                                            src=b,\n",
    "                                            dest=b_buffer)]\n",
    "        \n",
    "    #running program\n",
    "    kernel_event = kernel(queue,\n",
    "                          (M,M), #global size\n",
    "                          None, #local size \n",
    "                          a_buffer,b_buffer,c_buffer,M,N, #Kernel Arguments\n",
    "                          wait_for = copyon_events)\n",
    "        \n",
    "        \n",
    "    #copying data off device\n",
    "    c = numpy.empty((M,M),dtype=dt)\n",
    "    copyoff_event = pyopencl.enqueue_copy(queue,\n",
    "                                          src = c_buffer,\n",
    "                                          dest = c,\n",
    "                                          wait_for = [kernel_event])\n",
    "    copyoff_event.wait()\n",
    "        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on both platforms and checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_arrays = create_arrays()\n",
    "nvidia_result = opencl_dot_product(data_arrays,nvidia_buffers,\n",
    "                                   nvidia_queue,nvidia_program.dot_product)\n",
    "intel_result = opencl_dot_product(data_arrays,intel_buffers,\n",
    "                                  intel_queue,intel_program.dot_product)\n",
    "\n",
    "a,b = data_arrays\n",
    "ref_c = a.dot(b)\n",
    "if( numpy.abs(ref_c - nvidia_result).sum() ): print(\"Error in NVIDIA result!\")\n",
    "if( numpy.abs(ref_c - intel_result).sum() ): print(\"Error in Intel result!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, visualising the different optimisations:\n",
    "1. Gathering timing data and calculating acceleration ($A$) : $A = \\frac{L_{NumPy}}{L_{OpenCL}} $\n",
    "2. Plotting the data\n",
    "\n",
    "*N.B.* How in the no stride case, we're using the columnwise layout for the B matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_arrays = create_arrays(datatype=dt)\n",
    "a,b = data_arrays\n",
    "\n",
    "# Getting reference result\n",
    "start = time.time()\n",
    "ref_c = a.dot(b)\n",
    "end = time.time()\n",
    "ref_time = end - start\n",
    "\n",
    "nvidia_times = []\n",
    "intel_times = []\n",
    "#Iterating over the kernels for the two different platforms\n",
    "for nvidia_kernel,intel_kernel in zip(\n",
    "    (nvidia_program.dot_product,nvidia_program.dot_product_cached,nvidia_program.dot_product_no_stride),\n",
    "    (intel_program.dot_product,intel_program.dot_product_cached,intel_program.dot_product_no_stride)):\n",
    "    # Generating data\n",
    "    data_arrays = create_arrays(b_colwise = nvidia_kernel == nvidia_program.dot_product_no_stride)\n",
    "    \n",
    "    # Benchmarking NVIDIA\n",
    "    nvidia_start = time.time()\n",
    "    nvidia_result = opencl_dot_product(data_arrays,nvidia_buffers,\n",
    "                                       nvidia_queue,nvidia_kernel)\n",
    "    nvidia_end = time.time()\n",
    "    \n",
    "    nvidia_times += [nvidia_end - nvidia_start]\n",
    "    \n",
    "    #Benchmarking Intel\n",
    "    intel_start = time.time()\n",
    "    intel_result = opencl_dot_product(data_arrays,intel_buffers,\n",
    "                                      intel_queue,intel_kernel)\n",
    "    intel_end = time.time()\n",
    "    \n",
    "    intel_times += [intel_end - intel_start]\n",
    "    \n",
    "nvidia_speedup = ref_time/numpy.array(nvidia_times)\n",
    "intel_speedup = ref_time/numpy.array(intel_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2)\n",
    "ax1,ax2 = axes\n",
    "\n",
    "ax1.bar(range(nvidia_speedup.size),nvidia_speedup,width=0.8,color='g',alpha=0.5)\n",
    "ax2.bar(range(intel_speedup.size),intel_speedup,width=0.8,color='b',alpha=0.5)\n",
    "\n",
    "ax1.set_title(\"NVIDIA\")\n",
    "ax2.set_title(\"Intel\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels([\"Unoptimised\",\"Cached Sum\",\"Colwise Storage\"])\n",
    "    ax.set_xticks(numpy.arange(intel_speedup.size)+0.4)\n",
    "    ax.set_ylabel(\"Speedup over NumPy\")\n",
    "    ax.grid(True)\n",
    "    \n",
    "fig.set_size_inches(10,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Challenge\n",
    "* Apply similar optimisations to the Java hash function (see helper code below)\n",
    "* Measure the performance improvement over a Python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Data and Memory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_arrays_from_file(filename, size_limit = int(5e6)):\n",
    "    \"\"\"Reads a text file into numpy arrays.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    filename -- the file to read from\n",
    "    \n",
    "    Returns:\n",
    "    data -- numpy.Array ASCII with a single entry containing all of \n",
    "            the non-whitespace characters in the file.\n",
    "    offsets -- numpy.Array containing an integer element \n",
    "                for each word's offset\n",
    "    lengths -- numpy.Array containing an integer element \n",
    "                for each word's length  \n",
    "    \"\"\"\n",
    "    with open(filename,\"r\") as data_file:\n",
    "        # Reading data from file, and making it into one big string\n",
    "        data = [line[:-1] for line in data_file]\n",
    "        data_string = \"\".join(data)\n",
    "    \n",
    "        # Getting the lengths of each word\n",
    "        lengths = [len(word) for word in data]\n",
    "    \n",
    "        # Getting the start of each word\n",
    "        offsets = [0]\n",
    "        for word in data[:-1]:\n",
    "            offsets += [offsets[-1] + len(word)]\n",
    "        \n",
    "        # Testing that the offsets and counts are correct\n",
    "        for i,word in enumerate(data):\n",
    "            temp_word = data_string[offsets[i]:offsets[i]+lengths[i]] \n",
    "            if(word != temp_word): \n",
    "                print(\"Problem :\",word,\"!=\",temp_word)\n",
    "                raise(\"Data mismatch!\")\n",
    "            \n",
    "        # Converting data into numpy array\n",
    "        byte_data_string = data_string.encode(\"ascii\",\"ignore\")\n",
    "        data_array = numpy.array(byte_data_string)\n",
    "        offsets_array = numpy.array(offsets,dtype=numpy.int32)\n",
    "        lengths_array = numpy.array(lengths,dtype=numpy.int32)\n",
    "            \n",
    "    return data_array,offsets_array,lengths_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_data_buffers(context,words,offsets,lengths):\n",
    "    \"\"\"Creates a read only PyOpenCL buffers for a numpy.Array\n",
    "    \n",
    "    Keyword arguments:\n",
    "    context -- pyopencl.Context that buffer will be created in.\n",
    "    words -- numpy.Array of words data\n",
    "    offsets -- numpy.Array of offsets data\n",
    "    lengths -- numpy.Array of lengths data\n",
    "    \n",
    "    Returns:\n",
    "    words_buffers -- pyopencl.Buffer in context for words array\n",
    "    offsets_buffers -- pyopencl.Buffer in context for offsets array\n",
    "    lengths_buffers -- pyopencl.Buffer in context for lengths array\n",
    "    \"\"\"\n",
    "    ro_mem_flags = pyopencl.mem_flags.READ_ONLY | pyopencl.mem_flags.ALLOC_HOST_PTR\n",
    "    \n",
    "    # Source buffers\n",
    "    buffers = [pyopencl.Buffer(context,\n",
    "                               flags=ro_mem_flags,\n",
    "                               size=array.nbytes) \n",
    "               for array in (words,offsets,lengths)]\n",
    "    \n",
    "    wo_mem_flags = pyopencl.mem_flags.WRITE_ONLY\n",
    "    output_size = offsets.nbytes #int per word\n",
    "    \n",
    "    # Destination buffers\n",
    "    buffers += [pyopencl.Buffer(context,\n",
    "                                flags=wo_mem_flags,\n",
    "                                size=output_size)]\n",
    "    \n",
    "    return buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://s3-eu-west-1.amazonaws.com/word-count-test-bucket/english_1000_most_common.txt\n",
    "!wget https://s3-eu-west-1.amazonaws.com/word-count-test-bucket/wiki-100k.txt\n",
    "!wget https://s3-eu-west-1.amazonaws.com/word-count-test-bucket/shakespeare_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Source data\n",
    "words,offsets,lengths = get_word_arrays_from_file(\"shakespeare_words.txt\")\n",
    "arrays = words,offsets,lengths\n",
    "\n",
    "# NVIDIA Buffers\n",
    "nvidia_buffers = create_data_buffers(nvidia_context,words,offsets,lengths)\n",
    "nvidia_words_buffer,nvidia_offesets_buffer,nvidia_lengths_buffer,nvidia_hashes_buffer = nvidia_buffers\n",
    "\n",
    "# Intel Buffers\n",
    "intel_buffers = create_data_buffers(intel_context,words,offsets,lengths)\n",
    "intel_words_buffer,intel_offesets_buffer,intel_lengths_buffer,intel_hashes_buffer = intel_buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling and running the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "program_source_filename = \"part_2_challenge.cl\"\n",
    "\n",
    "with open(program_source_filename,\"r\") as program_source_file: \n",
    "    program_source = program_source_file.read()\n",
    "\n",
    "    nvidia_program_source,intel_program_source = [pyopencl.Program(context,program_source) \n",
    "                                                  for context in (nvidia_context,intel_context)]\n",
    "    \n",
    "nvidia_program,intel_program = [program.build() for program \n",
    "                                in (nvidia_program_source,intel_program_source)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hash_words(queue,program,data_arrays,data_buffers):\n",
    "    \"\"\"Copy data onto OpenCL device, \n",
    "    \n",
    "    Keyword arguments:\n",
    "    queue -- pyopencl.Queue \n",
    "    program -- compiled pyopencl.Program \n",
    "    data_arrays -- Source data arrays\n",
    "    data_buffers -- pyopencl.Buffers to use in pyopencl problem\n",
    "    hashes -- Empty numpy.Array for results\n",
    "    \n",
    "    Returns:\n",
    "    hashes -- Copy of numpy.Array of ints, containing the results\n",
    "    \"\"\"\n",
    "    words,offsets,lengths = data_arrays\n",
    "    words_buffer,offsets_buffer,lengths_buffer,hashes_buffer = data_buffers\n",
    "    \n",
    "    # Copying data onto the device\n",
    "                     # Words\n",
    "    copyon_events = [pyopencl.enqueue_copy(queue,\n",
    "                                           src=words,\n",
    "                                           dest=words_buffer),\n",
    "                     # Offsets\n",
    "                     pyopencl.enqueue_copy(queue,\n",
    "                                           src=offsets,\n",
    "                                           dest=offsets_buffer),\n",
    "                     # Lengths\n",
    "                     pyopencl.enqueue_copy(queue,\n",
    "                                           src=lengths,\n",
    "                                           dest=lengths_buffer)]\n",
    "    \n",
    "    # Hashing the words\n",
    "    kernel_event = program.java_hash(queue,\n",
    "                                     (offsets.size,), #global size\n",
    "                                     None, #local size\n",
    "                                     words_buffer,offsets_buffer,lengths_buffer,hashes_buffer, #buffers\n",
    "                                     wait_for=copyon_events)\n",
    "    \n",
    "    \n",
    "    # Copying data off the device\n",
    "    hashes = numpy.empty(offsets.shape,dtype=numpy.int32)\n",
    "    copyoff_event = pyopencl.enqueue_copy(queue,\n",
    "                                          src=hashes_buffer,\n",
    "                                          dest=hashes,\n",
    "                                          wait_for=[kernel_event])\n",
    "    \n",
    "    #wait for copy-off to finish\n",
    "    copyoff_event.wait()\n",
    "    \n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_queue = pyopencl.CommandQueue(nvidia_context)\n",
    "nvidia_hashes = hash_words(nvidia_queue,nvidia_program,arrays,nvidia_buffers)\n",
    "\n",
    "intel_queue = pyopencl.CommandQueue(intel_context)\n",
    "intel_hashes = hash_words(intel_queue,intel_program,arrays,intel_buffers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ref_java_hash(s):\n",
    "    \"\"\"Hashes a string into 32-bit integer value, as per the Java hash algorithm\"\"\"\n",
    "    h = 0\n",
    "    for c in s.encode(\"ascii\"):\n",
    "        h = (31 * h + c) & 0xFFFFFFFF\n",
    "        \n",
    "    return ((h + 0x80000000) & 0xFFFFFFFF) - 0x80000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_words = str(words)[2:-1]\n",
    "temp_words = [temp_words[offset:offset+length] for offset,length in zip(offsets,lengths)]\n",
    "ref_hashes = numpy.fromiter(map(ref_java_hash,temp_words),numpy.int32)\n",
    "\n",
    "i = 0\n",
    "for intel_hash,nvidia_hash,ref_hash in zip(intel_hashes,nvidia_hashes,ref_hashes):\n",
    "    if(nvidia_hash != ref_hash or intel_hash != ref_hash): \n",
    "        print(i,\"Problem!\",nvidia_hash,intel_hash,ref_hash)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit hash_words(nvidia_queue,nvidia_program,arrays,nvidia_buffers)\n",
    "%timeit hash_words(intel_queue,intel_program,arrays,intel_buffers)\n",
    "%timeit numpy.fromiter(map(ref_java_hash,temp_words),numpy.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
