{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Part 3 - Doing much task wow (with OpenCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Library Import\n",
    "As before, we need to import [PyOpenCL](https://documen.tician.de/pyopencl/) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyopencl,numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up platforms, devices and context\n",
    "We're going to setup the devices and context as explicit objects because we might want to interogate their runtime information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "platforms = pyopencl.get_platforms()\n",
    "nvidia_device,intel_device = [platform.get_devices()[0] \n",
    "                              for platform in platforms]\n",
    "nvidia_context,intel_context = [pyopencl.Context(devices=[device]) \n",
    "                                for device in (nvidia_device,intel_device)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Device Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the runtime API\n",
    "1. Selecting the properties of interest\n",
    "2. print out for each device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_properties = {\n",
    "    \"Device Name\":pyopencl.device_info.NAME,\n",
    "    \"Device Platform\":pyopencl.device_info.PLATFORM,\n",
    "    \"Device Type\":pyopencl.device_info.TYPE\n",
    "}\n",
    "\n",
    "processing_properties = {\n",
    "    \"Available Compute Units\": pyopencl.device_info.MAX_COMPUTE_UNITS,\n",
    "    \"Clockrate\": pyopencl.device_info.MAX_CLOCK_FREQUENCY\n",
    "}\n",
    "\n",
    "memory_properties = {\n",
    "    \"Available Global Memory\": pyopencl.device_info.GLOBAL_MEM_SIZE,\n",
    "    \"Available Constant Memory\": pyopencl.device_info.MAX_CONSTANT_BUFFER_SIZE,\n",
    "    \"Available Local Memory\" : pyopencl.device_info.LOCAL_MEM_SIZE\n",
    "}\n",
    "\n",
    "device_types = {\n",
    "    pyopencl.device_type.CPU:\"CPU\",\n",
    "    pyopencl.device_type.GPU:\"GPU\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for device in (nvidia_device,intel_device):\n",
    "    #print out all of the device name properties, except the device type\n",
    "    for property_name in sorted(name_properties.keys() - {\"Device Type\"}):\n",
    "        property_string_args = (property_name,device.get_info(name_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "        \n",
    "    #look up the device type\n",
    "    print(\"Device Types: %s\"%device_types[device.get_info(name_properties[\"Device Type\"])])\n",
    "    \n",
    "    #print out all of the processing properties\n",
    "    for property_name in sorted(processing_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(processing_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "    \n",
    "    #print out all of the memory properties\n",
    "    for property_name in sorted(memory_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(memory_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using clinfo (external application)\n",
    "Rather helpfully, Jupypter lets us run command line applications, including [clinfo](http://manpages.ubuntu.com/manpages/xenial/man1/clinfo.1.html), a command line utility for inspecting OpenCL devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task vs Data Parallelism\n",
    "### Setting up the program\n",
    "1. Create a program for an expensive element-wise operation\n",
    "2. Compile the programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"part_3.cl\",\"r\") as program_source_file:\n",
    "    program_source = program_source_file.read()\n",
    "\n",
    "nvidia_program_source,intel_program_source = [pyopencl.Program(context,program_source) \n",
    "                                              for context in (nvidia_context,intel_context)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_program,intel_program = [program.build(options=[\"-cl-fast-relaxed-math\"])\n",
    "                                for program in (nvidia_program_source,intel_program_source)]\n",
    "\n",
    "nvidia_queue = pyopencl.CommandQueue(nvidia_context)\n",
    "intel_queue = pyopencl.CommandQueue(intel_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the global memory resource\n",
    "1. Defining source data parameters\n",
    "2. Creating the source data\n",
    "3. Creating the memory resources within the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_arrays(M=2**12,N=2**10,datatype=numpy.int64):\n",
    "    #Setting up memory\n",
    "    a = (numpy.random.randint(0,10,size=(M,N))).astype(dt)\n",
    "    b = numpy.asfortranarray(numpy.random.randint(0,10,size=(N,M))).astype(dt)\n",
    "    \n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_buffers(context,datatype_size,M=2**12,N=2**10):\n",
    "    ro_mem_flags = pyopencl.mem_flags.READ_ONLY\n",
    "    \n",
    "    a_buffer = pyopencl.Buffer(context,\n",
    "                               flags=ro_mem_flags,\n",
    "                               size=M*N*datatype_size)              \n",
    "    b_buffer = pyopencl.Buffer(context,\n",
    "                               flags=ro_mem_flags,\n",
    "                               size=M*N*datatype_size)\n",
    "    \n",
    "    # Local buffer is size of column\n",
    "    local_N = N\n",
    "    local_b_buffer = pyopencl.LocalMemory(datatype_size*local_N)\n",
    "    \n",
    "    wo_mem_flags = pyopencl.mem_flags.WRITE_ONLY\n",
    "    c_buffer = pyopencl.Buffer(context,\n",
    "                               flags=wo_mem_flags,\n",
    "                               size=M*M*datatype_size)\n",
    "    \n",
    "    return a_buffer,b_buffer,c_buffer,local_b_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating buffers\n",
    "dt = numpy.float32 #The datatype we're using - this should match up with the source code\n",
    "data_arrays = create_arrays(datatype=dt)\n",
    "\n",
    "datatype_size = numpy.dtype(dt).itemsize\n",
    "nvidia_buffers = create_buffers(nvidia_context,datatype_size)\n",
    "intel_buffers = create_buffers(intel_context,datatype_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the program\n",
    "### Defining the host program\n",
    "Similar to how we did it in module 2, but now we are *setting the workgroup size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opencl_dot_product(data_arrays,buffers,queue,kernel,\n",
    "                       M=2**12,N=2**10,max_wg_size=2**10):\n",
    "    a,b = data_arrays\n",
    "    a_buffer,b_buffer,c_buffer,local_b_buffer = buffers\n",
    "    M,N = numpy.int32(M),numpy.int32(N) #converting to the exepcted type for OpenCL\n",
    "    \n",
    "    #copying data onto device\n",
    "    copyon_events = []\n",
    "        \n",
    "    copyon_events += [pyopencl.enqueue_copy(queue,\n",
    "                                            src=a,\n",
    "                                            dest=a_buffer)]\n",
    "    copyon_events += [pyopencl.enqueue_copy(queue,\n",
    "                                            src=b,\n",
    "                                            dest=b_buffer)]\n",
    "    \n",
    "    # Finding the right work group size\n",
    "    # We can't go above the GPU max (1024),\n",
    "    # but we should use all compute units\n",
    "    work_group_size = min((M,max_wg_size))\n",
    "    while(M/work_group_size < 13): work_group_size /= 2 \n",
    "    work_group_size = int(work_group_size)\n",
    "        \n",
    "    #running program\n",
    "    kernel_event = kernel(queue,\n",
    "                          (M,M), #global size\n",
    "                          (work_group_size,1), #local size - all rows, just one column wg_size\n",
    "                          a_buffer,b_buffer,c_buffer,local_b_buffer,M,N, #Kernel Arguments\n",
    "                          wait_for = copyon_events)\n",
    "        \n",
    "    #copying data off device\n",
    "    c = numpy.empty((M,M),dtype=dt)\n",
    "    copyoff_event = pyopencl.enqueue_copy(queue,\n",
    "                                          src = c_buffer,\n",
    "                                          dest = c,\n",
    "                                          wait_for = [kernel_event])\n",
    "    copyoff_event.wait()\n",
    "        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_arrays = create_arrays()\n",
    "nvidia_result = dot_product(data_arrays,nvidia_buffers,\n",
    "                            nvidia_queue,nvidia_program.dot_product_cc)\n",
    "intel_result = dot_product(data_arrays,intel_buffers,\n",
    "                           intel_queue,intel_program.dot_product_cc)\n",
    "\n",
    "a,b = data_arrays\n",
    "ref_c = a.dot(b)\n",
    "if( numpy.abs(ref_c - nvidia_result).sum() ): print(\"Error in NVIDIA result!\",numpy.abs(ref_c - nvidia_result).sum())\n",
    "if( numpy.abs(ref_c - intel_result).sum() ): print(\"Error in Intel result!\",numpy.abs(ref_c - intel_result).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit a.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit dot_product(data_arrays,nvidia_buffers,nvidia_queue,nvidia_program.dot_product)\n",
    "%timeit dot_product(data_arrays,nvidia_buffers,nvidia_queue,nvidia_program.dot_product_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit dot_product(data_arrays,intel_buffers,intel_queue,intel_program.dot_product)\n",
    "%timeit dot_product(data_arrays,intel_buffers,intel_queue,intel_program.dot_product_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Challenge\n",
    "Take the above code and characterise performance across:\n",
    "* Different `M` and `N` values. \n",
    "* datatypes: `int, long, float, double`.\n",
    "\n",
    "Convince yourself (and your partner!) as to what explains the different performance characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
