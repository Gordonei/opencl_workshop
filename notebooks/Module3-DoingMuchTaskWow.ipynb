{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Module 3 - Doing much task wow (with OpenCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Library Import\n",
    "Before doing anything else, we need to import [PyOpenCL](https://documen.tician.de/pyopencl/) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyopencl,numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up platforms, devices and context\n",
    "We're going to setup the devices and context as explicit objects because we might want to interogate their runtime information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "platforms = pyopencl.get_platforms()\n",
    "nvidia_device,intel_device = [platform.get_devices()[0] \n",
    "                              for platform in platforms]\n",
    "nvidia_context,intel_context = [pyopencl.Context(devices=[device]) \n",
    "                                for device in (nvidia_device,intel_device)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Device Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the runtime API\n",
    "1. Selecting the properties of interest\n",
    "2. print out for each device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_properties = {\n",
    "    \"Device Name\":pyopencl.device_info.NAME,\n",
    "    \"Device Platform\":pyopencl.device_info.PLATFORM,\n",
    "    \"Device Type\":pyopencl.device_info.TYPE\n",
    "}\n",
    "\n",
    "processing_properties = {\n",
    "    \"Available Compute Units\": pyopencl.device_info.MAX_COMPUTE_UNITS,\n",
    "    \"Clockrate\": pyopencl.device_info.MAX_CLOCK_FREQUENCY\n",
    "}\n",
    "\n",
    "memory_properties = {\n",
    "    \"Available Global Memory\": pyopencl.device_info.GLOBAL_MEM_SIZE,\n",
    "    \"Available Constant Memory\": pyopencl.device_info.MAX_CONSTANT_BUFFER_SIZE,\n",
    "    \"Available Local Memory\" : pyopencl.device_info.LOCAL_MEM_SIZE\n",
    "}\n",
    "\n",
    "device_types = {\n",
    "    pyopencl.device_type.CPU:\"CPU\",\n",
    "    pyopencl.device_type.GPU:\"GPU\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for device in (nvidia_device,intel_device):\n",
    "    #print out all of the device name properties, except the device type\n",
    "    for property_name in sorted(name_properties.keys() - {\"Device Type\"}):\n",
    "        property_string_args = (property_name,device.get_info(name_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "        \n",
    "    #look up the device type\n",
    "    print(\"Device Types: %s\"%device_types[device.get_info(name_properties[\"Device Type\"])])\n",
    "    \n",
    "    #print out all of the processing properties\n",
    "    for property_name in sorted(processing_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(processing_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "    \n",
    "    #print out all of the memory properties\n",
    "    for property_name in sorted(memory_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(memory_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using clinfo (external application)\n",
    "Rather helpfully, Jupypter lets us run command line applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task vs Data Parallelism\n",
    "### Setting up the program\n",
    "1. Create a program for Vector element-wise multiplication\n",
    "2. Compile the programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "program_source = \"\"\"\n",
    "kernel void operation(global double *a,\n",
    "                      global double *b,\n",
    "                      global double *c)\n",
    "{\n",
    "  int gid = get_global_id(0);\n",
    "  \n",
    "  double a_temp = a[gid];\n",
    "  double b_temp = b[gid];\n",
    "  double result = (b_temp + a_temp)*(b_temp*a_temp) + (b_temp - a_temp)*(b_temp/a_temp);\n",
    "  \n",
    "  c[gid] = result*result;\n",
    "}\n",
    "\"\"\"\n",
    "nvidia_program_source,intel_program_source = [pyopencl.Program(context,program_source) \n",
    "                                              for context in (nvidia_context,intel_context)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_program,intel_program = [program.build()\n",
    "                                for program in (nvidia_program_source,intel_program_source)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the global memory resource\n",
    "1. Defining source data parameters\n",
    "2. Creating the source data\n",
    "3. Creating the memory resources within the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 100\n",
    "N = int(8*2**15)\n",
    "dt = numpy.float64\n",
    "dt_size = numpy.dtype(dt).itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = numpy.random.random((M,N)).astype(dt)+0.01 #make sure it's not zero\n",
    "b = numpy.random.random((M,N)).astype(dt)*a\n",
    "c = numpy.empty_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_buffers(context,a_size,b_size,c_size):\n",
    "    a_buffer = pyopencl.Buffer(context,\n",
    "                               flags = pyopencl.mem_flags.READ_ONLY | pyopencl.mem_flags.ALLOC_HOST_PTR, \n",
    "                               size=a_size)\n",
    "    b_buffer = pyopencl.Buffer(context, \n",
    "                               flags=pyopencl.mem_flags.READ_ONLY | pyopencl.mem_flags.ALLOC_HOST_PTR, \n",
    "                               size=b_size)\n",
    "    c_buffer = pyopencl.Buffer(context,\n",
    "                              flags=pyopencl.mem_flags.WRITE_ONLY | pyopencl.mem_flags.ALLOC_HOST_PTR,\n",
    "                              size=c_size)\n",
    "    return a_buffer,b_buffer,c_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvidia_a_buffer,nvidia_b_buffer,nvidia_c_buffer = create_buffers(nvidia_context,N*dt_size,N*dt_size,N*dt_size)\n",
    "intel_a_buffer,intel_b_buffer,intel_c_buffer = create_buffers(intel_context,N*dt_size,N*dt_size,N*dt_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the program\n",
    "### Defining the host program\n",
    "Similar to how we did it in module 2, but now *we* are setting the workgroup size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copyon(queue,a_row,a_buffer,b_row,b_buffer):\n",
    "    copyon_events = []\n",
    "        \n",
    "    copyon_events += [pyopencl.enqueue_copy(queue,\n",
    "                                            src=a_row,\n",
    "                                            dest=a_buffer,\n",
    "                                            is_blocking = False)]\n",
    "    copyon_events += [pyopencl.enqueue_copy(queue,\n",
    "                                            src=b_row,\n",
    "                                            dest=b_buffer,\n",
    "                                            is_blocking = False)]\n",
    "    return copyon_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_kernel(queue,program,work_items,wg_size,a_buffer,b_buffer,c_buffer,copyon_events):\n",
    "    kernel_event = program.operation(queue,\n",
    "                                     (work_items,), #global size\n",
    "                                     wg_size, #local size\n",
    "                                     a_buffer,b_buffer,c_buffer,\n",
    "                                     wait_for = copyon_events)\n",
    "    return [kernel_event]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copyoff(queue,c_buffer,c_row,kernel_events):\n",
    "    c_row = numpy.empty_like(c_row)\n",
    "    copyoff_event = pyopencl.enqueue_copy(queue,\n",
    "                                          src = c_buffer,\n",
    "                                          dest = c_row,\n",
    "                                          wait_for = kernel_events,\n",
    "                                          is_blocking = False)\n",
    "    return [copyoff_event],c_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_norm(queue,a,a_buffer,b,b_buffer,c_buffer,program,wgs):\n",
    "    c = numpy.empty_like(a)\n",
    "    total = 0.0\n",
    "    \n",
    "    if(wgs!=None): wg_size = (int(a.shape[1]/wgs),)\n",
    "    else: wg_size = None\n",
    "        \n",
    "    for i,(a_row,b_row,c_row) in enumerate(zip(a,b,c)):\n",
    "        #copying data onto device\n",
    "        copyon_events = copyon(queue,a_row,a_buffer,b_row,b_buffer)\n",
    "        \n",
    "        #running program\n",
    "        kernel_events = run_kernel(queue,program,a_row.shape[0],wg_size,a_buffer,b_buffer,c_buffer,copyon_events)\n",
    "        \n",
    "        #copying data off device\n",
    "        copyoff_event,c[i] = copyoff(queue,c_buffer,c_row,kernel_events)\n",
    "        \n",
    "        #since we might as well do something useful while we wait\n",
    "        if(i>0): total += c[i-1].sum()\n",
    "            \n",
    "        #wait for copy-off to finish\n",
    "        copyoff_event[0].wait()\n",
    "            \n",
    "    total += c[-1].sum()\n",
    "        \n",
    "    return total**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up work queue\n",
    "1. Creating out of order execution queue.\n",
    "2. Compute norm using as many work groups as there are compute units.\n",
    "3. Compare to a reference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_oo_queue = pyopencl.CommandQueue(nvidia_context,\n",
    "                                        properties = pyopencl.command_queue_properties.OUT_OF_ORDER_EXEC_MODE_ENABLE)\n",
    "intel_oo_queue = pyopencl.CommandQueue(intel_context,\n",
    "                                       properties = pyopencl.command_queue_properties.OUT_OF_ORDER_EXEC_MODE_ENABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nvidia_oo_norm = compute_norm(nvidia_oo_queue,\n",
    "                              a,nvidia_a_buffer,\n",
    "                              b,nvidia_b_buffer,\n",
    "                              nvidia_c_buffer,\n",
    "                              nvidia_program,\n",
    "                              None\n",
    "                             )\n",
    "\n",
    "intel_oo_norm = compute_norm(intel_oo_queue,\n",
    "                             a,intel_a_buffer,\n",
    "                             b,intel_b_buffer,\n",
    "                             intel_c_buffer,\n",
    "                             intel_program,\n",
    "                             None\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_result = numpy.linalg.norm((b+a)*(b*a) + (b-a)*(b/a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(numpy.abs(reference_result - nvidia_oo_norm) > 22): raise Exception(\"nvidia result does not match!\")\n",
    "if(numpy.abs(reference_result - intel_oo_norm) > 22): raise Exception(\"intel result does not match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "1. Create a function for benchmarking performance of the kernel (without the memory copy)\n",
    "2. Benchmark between the 64 ($2^6$) and 4096 ($2^{12}$) workgroups\n",
    "3. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_program(queue,a,a_buffer,b,b_buffer,c_buffer,\n",
    "                     program,device,\n",
    "                     multiple,n=100):\n",
    "    \n",
    "    run_time = 0\n",
    "    for i in range(n):\n",
    "        #setup for running the kernel\n",
    "        copyon_events = copyon(queue,a[i],a_buffer,b[i],b_buffer)\n",
    "        \n",
    "        cus = device.get_info(pyopencl.device_info.MAX_COMPUTE_UNITS)\n",
    "        wgs = multiple*cus\n",
    "        wg_size = (int(a.shape[1]/wgs),)\n",
    "        \n",
    "        #run kernel\n",
    "        start = time.time()\n",
    "        run_kernel(queue,program,a[i].shape[0],wg_size,a_buffer,b_buffer,c_buffer,copyon_events)\n",
    "        #compute_norm(queue,a,a_buffer,b,b_buffer,c_buffer,program,multiple*cus)\n",
    "        stop = time.time()\n",
    "        \n",
    "        run_time += stop - start\n",
    "    \n",
    "    return run_time/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_wgs = 5\n",
    "max_wgs = 16\n",
    "\n",
    "print(\"wg\\tGPU\\t\\tCPU\")\n",
    "nvidia_times = []\n",
    "intel_times = []\n",
    "for t in range(min_wgs,max_wgs):\n",
    "    nvidia_times += [evaluate_program(nvidia_oo_queue,a,nvidia_a_buffer,b,nvidia_b_buffer,nvidia_c_buffer,\n",
    "                                      nvidia_program,nvidia_device,\n",
    "                                      2**t)]\n",
    "    intel_times += [evaluate_program(intel_oo_queue,a,intel_a_buffer,b,intel_b_buffer,intel_c_buffer,\n",
    "                                     intel_program,intel_device,\n",
    "                                     2**t)]\n",
    "    \n",
    "    print(\"%d:\\t%.6f\\t%.6f\"%(2**t,nvidia_times[-1],intel_times[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequential_time = 0\n",
    "for i in range(100):\n",
    "    start = time.time()\n",
    "    result = (b[i]+a[i])*(b[i]*a[i]) + (b[i]-a[i])*(b[i]/a[i])\n",
    "    end = time.time()\n",
    "    \n",
    "    sequential_time += end-start\n",
    "    \n",
    "sequential_time = sequential_time/10\n",
    "print(\"seq:\\t%.6f\"%sequential_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(2**numpy.arange(min_wgs,max_wgs),intel_times,label=\"Intel CPU\")\n",
    "plt.plot(2**numpy.arange(min_wgs,max_wgs),nvidia_times,label=\"NVIDIA GPU\")\n",
    "plt.hlines(sequential_time,color='r',xmin=2**5,xmax=2**15)\n",
    "\n",
    "plt.xlim(2**min_wgs,2**(max_wgs-1))\n",
    "plt.xticks(2**numpy.arange(min_wgs,max_wgs))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Number of Workgroups')\n",
    "plt.ylabel('Kernel Time (S)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Challenge\n",
    "* Perform any BLAS operation, using a mixture of task and data parallelism\n",
    "* Characterise the change in any of the values\n",
    "\n",
    "*Hint: Take advantage of multiple indices.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
