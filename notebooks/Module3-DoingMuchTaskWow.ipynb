{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Module 3 - Doing much task wow (with OpenCL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Library Import\n",
    "Before doing anything else, we need to import [PyOpenCL](https://documen.tician.de/pyopencl/) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyopencl,numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up platforms, devices and context\n",
    "We're going to setup the devices and context as explicit objects because we might want to interogate their runtime information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "platforms = pyopencl.get_platforms()\n",
    "nvidia_device,intel_device = [platform.get_devices()[0] \n",
    "                              for platform in platforms]\n",
    "nvidia_context,intel_context = [pyopencl.Context(devices=[device]) \n",
    "                                for device in (nvidia_device,intel_device)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Device Properties\n",
    "1. Selecting the properties of interest\n",
    "2. print out for each device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_properties = {\n",
    "    \"Device Name\":pyopencl.device_info.NAME,\n",
    "    \"Device Platform\":pyopencl.device_info.PLATFORM,\n",
    "    \"Device Type\":pyopencl.device_info.TYPE\n",
    "}\n",
    "\n",
    "processing_properties = {\n",
    "    \"Available Compute Units\": pyopencl.device_info.MAX_COMPUTE_UNITS,\n",
    "    \"Clockrate\": pyopencl.device_info.MAX_CLOCK_FREQUENCY\n",
    "}\n",
    "\n",
    "memory_properties = {\n",
    "    \"Available Global Memory\": pyopencl.device_info.GLOBAL_MEM_SIZE,\n",
    "    \"Available Constant Memory\": pyopencl.device_info.MAX_CONSTANT_BUFFER_SIZE,\n",
    "    \"Available Local Memory\" : pyopencl.device_info.LOCAL_MEM_SIZE\n",
    "}\n",
    "\n",
    "device_types = {\n",
    "    pyopencl.device_type.CPU:\"CPU\",\n",
    "    pyopencl.device_type.GPU:\"GPU\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for device in (nvidia_device,intel_device):\n",
    "    for property_name in sorted(name_properties.keys() - {\"Device Type\"}):\n",
    "        property_string_args = (property_name,device.get_info(name_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "        \n",
    "    print(\"Device Types: %s\"%device_types[device.get_info(name_properties[\"Device Type\"])])\n",
    "    \n",
    "    for property_name in sorted(processing_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(processing_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "    \n",
    "    for property_name in sorted(memory_properties.keys()):\n",
    "        property_string_args = (property_name,device.get_info(memory_properties[property_name]))\n",
    "        print(\"%s: %s\"%property_string_args)\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task vs Data Parallelism\n",
    "### Setting up the program\n",
    "1. Create a program for Vector element-wise multiplication\n",
    "2. Compile the programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "program_source = \"\"\"\n",
    "kernel void operation(global long *a,\n",
    "                      global long *b)\n",
    "{\n",
    "  int gid = get_global_id(0);\n",
    "  \n",
    "  long a_temp = a[gid];\n",
    "  long b_temp = b[gid];\n",
    "  \n",
    "  b[gid] = b_temp/a_temp + b_temp*a_temp - b_temp%a_temp;\n",
    "}\n",
    "\"\"\"\n",
    "nvidia_program_source,intel_program_source = [pyopencl.Program(context,program_source) \n",
    "                                              for context in (nvidia_context,intel_context)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_program,intel_program = [program.build()\n",
    "                                for program in (nvidia_program_source,intel_program_source)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the global memory resource\n",
    "1. Defining source data parameters\n",
    "2. Creating the source data\n",
    "3. Creating the memory resources within the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = 10\n",
    "N = int(128e3)\n",
    "dt = numpy.int64\n",
    "dt_size = numpy.dtype(dt).itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = numpy.random.randint(low=1,high=10,size=(M,N)).astype(dt)\n",
    "b = numpy.random.randint(low=1,high=1000,size=(M,N)).astype(dt)*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_buffers(context,a_size,b_size):\n",
    "    a_buffer = pyopencl.Buffer(context,\n",
    "                               flags = pyopencl.mem_flags.READ_ONLY | pyopencl.mem_flags.ALLOC_HOST_PTR, \n",
    "                               size=a_size)\n",
    "    b_buffer = pyopencl.Buffer(context, \n",
    "                               flags=pyopencl.mem_flags.READ_WRITE | pyopencl.mem_flags.ALLOC_HOST_PTR, \n",
    "                               size=b_size)\n",
    "    return a_buffer,b_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvidia_a_buffer,nvidia_b_buffer = create_buffers(nvidia_context,N*dt_size,N*dt_size)\n",
    "intel_a_buffer,intel_b_buffer = create_buffers(intel_context,N*dt_size,N*dt_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the program\n",
    "### Defining the host program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_norm(queue,a,a_buffer,b,b_buffer,program,wgs):\n",
    "    c = numpy.empty_like(a)\n",
    "    total = 0.0\n",
    "    \n",
    "    wg_size = int(a.shape[0]/wgs)\n",
    "    for i,(a_row,b_row) in enumerate(zip(a,b)):\n",
    "        #copying data onto device\n",
    "        copyon_events = []\n",
    "        \n",
    "        copyon_events += [pyopencl.enqueue_copy(queue,\n",
    "                                                src=a_row,\n",
    "                                                dest=a_buffer,\n",
    "                                                is_blocking = False)]\n",
    "        copyon_events += [pyopencl.enqueue_copy(queue,\n",
    "                                                src=b_row,\n",
    "                                                dest=b_buffer,\n",
    "                                                is_blocking = False)]\n",
    "        \n",
    "        #running program\n",
    "        kernel_event = program.operation(queue,\n",
    "                                         a_row.shape, #global size\n",
    "                                         (wg_size,), #local size\n",
    "                                         a_buffer,b_buffer,\n",
    "                                         wait_for = copyon_events)\n",
    "        \n",
    "        kernel_event2 = program.square(queue,\n",
    "                                       b_row.shape, #global size\n",
    "                                       (wg_size,), #local size\n",
    "                                       b_buffer,\n",
    "                                       wait_for = [kernel_event])\n",
    "        \n",
    "        #copying data off device\n",
    "        copyoff_event = pyopencl.enqueue_copy(queue,\n",
    "                                              src = b_buffer,\n",
    "                                              dest = c[i],\n",
    "                                              wait_for = [kernel_event2],\n",
    "                                              is_blocking = False)\n",
    "        \n",
    "        #since we might as well do something useful while we wait\n",
    "        if(i>0): total += c[i-1].sum()\n",
    "            \n",
    "        #wait for copy-off to finish\n",
    "        copyoff_event.wait()\n",
    "        \n",
    "    total += c[-1].sum()\n",
    "        \n",
    "    return total**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-order Execution\n",
    "Similiar to before, but using out of order execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nvidia_oo_queue = pyopencl.CommandQueue(nvidia_context,\n",
    "                                        properties = pyopencl.command_queue_properties.OUT_OF_ORDER_EXEC_MODE_ENABLE)\n",
    "intel_oo_queue = pyopencl.CommandQueue(intel_context,\n",
    "                                       properties = pyopencl.command_queue_properties.OUT_OF_ORDER_EXEC_MODE_ENABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nvidia_oo_norm = compute_norm(nvidia_oo_queue,\n",
    "                              a,nvidia_a_buffer,\n",
    "                              b,nvidia_b_buffer,\n",
    "                              nvidia_program,\n",
    "                              2*nvidia_device.get_info(pyopencl.device_info.MAX_COMPUTE_UNITS)\n",
    "                             )\n",
    "    \n",
    "intel_oo_norm = compute_norm(intel_oo_queue,\n",
    "                             a,intel_a_buffer,\n",
    "                             b,intel_b_buffer,\n",
    "                             intel_program,\n",
    "                             2*intel_device.get_info(pyopencl.device_info.MAX_COMPUTE_UNITS)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reference_result = numpy.linalg.norm(b/a + b*a - b%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if(reference_result - nvidia_oo_norm > 0): raise Exception(\"nvidia result does not match!\")\n",
    "if(reference_result - intel_oo_norm > 0): raise Exception(\"intel result does not match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_program(nvidia_queue,intel_queue,threads,n=10):\n",
    "    nvidia_time = 0\n",
    "    intel_time = 0\n",
    "    for i in range(n):\n",
    "        nvidia_start = time.time()\n",
    "        compute_norm(nvidia_io_queue,a,nvidia_a_buffer,b,nvidia_b_buffer,nvidia_program)\n",
    "        nvidia_stop = time.time()\n",
    "        nvidia_time += nvidia_stop - nvidia_start\n",
    "    \n",
    "        intel_start = time.time()\n",
    "        compute_norm(intel_io_queue,a,intel_a_buffer,b,intel_b_buffer,intel_program)\n",
    "        intel_stop = time.time()\n",
    "        intel_time += intel_stop - intel_start\n",
    "    \n",
    "    return nvidia_time/n,intel_time/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in range(0,5):\n",
    "    nvidia_time,intel_time = evaluate_program(nvidia_oo_queue,intel_oo_queue,2**t,n=10)\n",
    "    print(\"t=%d: %.3f %.3f\"%(2**t,nvidia_time,intel_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_program(nvidia_oo_queue,intel_oo_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit -n 10 numpy.linalg.norm(b/a + b*a - b%a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Challenge\n",
    "* Perform any BLAS operation, using a mixture of task and data parallelism\n",
    "* Characterise the change in any of the values\n",
    "\n",
    "*Hint: Take advantage of multiple indices.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
